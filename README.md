# ğŸ‘¨â€ğŸ« From Problem-Solving to Teaching Problem-Solving  
### *Aligning LLMs with Pedagogy using Reinforcement Learning*
**What if your LLM could teachâ€”rather than just answer?**  
This project provides a recipe to transform a standard instruction-tuned language model into a math tutor that actually teaches, using multi-turn **GRPO** in a classroom environment with a **synthetic student**.

LLMs today excel at solving problems, but good teaching is about **strategically withholding answers** and guiding learners through their own reasoning. This repo implements a scalable, annotation-free framework that aligns LLM behavior with **pedagogical principles** like Socratic questioning, helpful scaffolding, and solution withholding.

We train 7B-sized open models to come close to closed-source models like LearnLMâ€”**without any human labels**â€”and maintain performance on reasoning tasks like GSM8K and MATH500.

> ğŸ”§ For implementation details (multi-node setup, memory handling, vLLM coordination), see [technical.md](technical.md).  
> ğŸ” Explore tutorâ€“student conversations here: [Conversation Visualizer](https://pedagogical-rl.vercel.app)

ğŸ“„ **Paper**: [arxiv.org/abs/2505.15607](https://arxiv.org/abs/2505.15607)  
ğŸ§  **Models**: We release two versions of our 7B tutor model:
- ğŸ¤— [eth-nlped/TutorRL-7B](https://huggingface.co/eth-nlped/TutorRL-7B) â€” standard version without internal planning  
- ğŸ¤— [eth-nlped/TutorRL-7B-think](https://huggingface.co/eth-nlped/TutorRL-7B-think) â€” enhanced with explicit `<think>` tags for interpretable planning

ğŸ”“ **License**: CC-BY-4.0


## ğŸ§  Core logic

The reinforcement learning loop is implemented in [`src/classroom.py`](src/classroom.py), which simulates a multi-turn dialog between:

- **Tutor (LLM under training)**
- **Student (frozen LLM)**
- **Judges (for leakage/helpfulness evaluation)**
- **Reward calculator (post-dialog solve rate & pedagogy)**

Each conversation follows a state machine:

```

START â†’ TEACHER_TURN â†’ STUDENT_TURN â†’ TEACHER_TURN â†’ ... â†’ JUDGE_TURN â†’ GENERATE_SOLUTION â†’ REWARD_TURN â†’ END

````

- The loop alternates between **tutor and student turns**
- **Student turns are masked** during loss calculation
- **Only the tutor model is updated** using GRPO
- **Conversations are processed in large batches**
- **Model weights are dynamically loaded/unloaded** across states to conserve memory

---

## ğŸš€ Quick start

```bash
# 1) create a clean environment
conda create -n pedagogy python=3.11
conda activate pedagogy

# 2) install core dependencies
pip install -r requirements.txt          # torch, trl, vllm, ...

# 3) add your keys
nano .env
````

---

## ğŸ§ª Environment variables

| Variable             | Purpose                             |
| -------------------- | ----------------------------------- |
| `HF_TOKEN`           | Pull/push models from ğŸ¤— Hub        |
| `WANDB_API_KEY`      | Weights & Biases logging (optional) |
| `OPENROUTER_API_KEY` | Optional: for LLMs via OpenRouter   |

---

## ğŸ“ Training

### 1 Â· Supervised Fine-Tuning (baseline)

```bash
accelerate launch \
  --config_file config/deepspeed/zero2_4GPU.yaml \
  train_sft.py --config-name 7b.yaml
```

### 2 Â· Pedagogical Reinforcement Learning (ours)

```bash
./start_rl_training.sh \
  --config_file config/deepspeed/zero3_4GPU.yaml \
  --config-name 7b.yaml
```

#### ğŸ§© Soft vs. Hard Pedagogy Mode

Use the `generation.ignore_rejected_judge` flag:

| Flag                          | Reward behavior                                             |
| ----------------------------- | ----------------------------------------------------------- |
| `ignore_rejected_judge=true`  | **Soft**: Apply a relative penalty `âˆ’Î»` if rejected         |
| `ignore_rejected_judge=false` | **Hard**: Override total reward with fixed `âˆ’Î»` if rejected |

ğŸ› Other common CLI knobs:

| Flag                                                   | Description                                     |
| ------------------------------------------------------ | ----------------------------------------------- |
| `generation.extra_penalty_for_rejected_judges`         | Î» â€” penalty magnitude                           |
| `generation.number_judge_attempts=0`                   | Î» = 0 (no pedagogy constraint)                  |
| `generation.use_thinking=true` / `force_thinking=true` | Enable hidden `<think>` tags for tutor planning |

---

## ğŸ“ˆ Evaluation

### Pedagogical Benchmarks

Evaluate on BigMath-style multi-turn dialogs:

```bash
python eval.py --config-name Qwen2.5-7B-Instruct.yaml
```

### Reasoning Benchmarks (MMLU, GSM8K, MATH500)

Using [LightEval](https://github.com/huggingface/lighteval):

```bash
lighteval vllm \
  "model_name=Qwen/Qwen2.5-7B-Instruct,gpu_memory_utilization=0.85,max_model_length=8192,dtype=bfloat16,\
   generation_parameters={max_new_tokens:8192,temperature:0.6,top_p:0.95}" \
  "lighteval|math_500|0|0,helm|mmlu|5|0,lighteval|gsm8k|4|0" \
  --use-chat-template
```

---

## ğŸ§± Project layout

```
.
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ deepspeed/            # ZeRO configs
â”‚   â”œâ”€â”€ eval/                 # eval task configs
â”‚   â””â”€â”€ train_rl/             # RL training configs
â”œâ”€â”€ prompt_templates/         # system + judge prompts
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ classroom.py          # ğŸ§  Core RL loop (multi-agent dialog)
â”‚   â”œâ”€â”€ grpo/                 # GRPO reinforcement learning trainer
â”‚   â”œâ”€â”€ inference_providers/  # OpenRouter / Gemini adapters
â”‚   â””â”€â”€ vllm/                 # vLLM helpers (multi-node, memory mgmt)
â”œâ”€â”€ utils/                    # reward functions, judge filters, logging
â”œâ”€â”€ train_rl.py               # Entry point for RL training
â”œâ”€â”€ train_sft.py              # Supervised fine-tuning entry
â””â”€â”€ start_rl_training.sh      # Launcher for RL
```

---

## ğŸ“„ Citation

```
@misc{dinucujianu2025problemsolvingteachingproblemsolvingaligning,
      title={From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning}, 
      author={David Dinucu-Jianu and Jakub Macina and Nico Daheim and Ido Hakimi and Iryna Gurevych and Mrinmaya Sachan},
      year={2025},
      eprint={2505.15607},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.15607}, 
}
```